{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citeseer dataset: #classes = 6 , #features = 3703 , #nodes = 3327 , #edges = 4676\n",
      "Feature set shapes (train, valid, test): (120, 3703) (500, 3703) (1000, 3703)\n",
      "Labels shapes (train, valid, test): (120, 6) (500, 6) (1000, 6)\n",
      "3.60685302074 15.0285542531 30.0571085062\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Semi-Supervised Classification of Graph Nodes using Exponential Decay\n",
    "# L42: Assessment 2\n",
    "# Jan Ondras (jo356), Trinity College\n",
    "######################################################################\n",
    "# Baseline MLP training, validation and testing, Citeseer dataset\n",
    "#############################################################################################################\n",
    "#############################################################################################################\n",
    "# Load data \n",
    "#############################################################################################################\n",
    "\n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from gcn.utils import *\n",
    "\n",
    "dataset_type = 'citeseer'\n",
    "\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(dataset_type)\n",
    "\n",
    "if dataset_type == 'citeseer':\n",
    "    N_classes = 6\n",
    "    N_features = 3703\n",
    "    N_nodes = 3327\n",
    "    N_edges = 4676 #4732 - incorrect in GCN paper !\n",
    "    if (N_classes != y_train.shape[1] or N_features != features.shape[1] or N_nodes != features.shape[0] \n",
    "            or N_edges != (np.sum(adj.toarray().diagonal()) + np.sum(adj.todense()))/2. ):\n",
    "        raise ValueError(\"Dataset dimensions differ from expected!\")\n",
    "else:\n",
    "    raise ValueError(\"Dataset not supported!\")\n",
    "print dataset_type, \"dataset:\", \"#classes =\", N_classes, \", #features =\", N_features, \", #nodes =\", N_nodes, \", #edges =\", N_edges\n",
    "\n",
    "X_train = features[train_mask].toarray()\n",
    "X_val = features[val_mask].toarray()\n",
    "X_test = features[test_mask].toarray()\n",
    "y_train = y_train[train_mask]\n",
    "y_val = y_val[val_mask]\n",
    "y_test = y_test[test_mask]\n",
    "\n",
    "print \"Feature set shapes (train, valid, test):\", X_train.shape, X_val.shape, X_test.shape\n",
    "print \"Labels shapes (train, valid, test):\", y_train.shape, y_val.shape, y_test.shape\n",
    "print y_train.shape[0]*100./N_nodes, y_val.shape[0]*100./N_nodes, y_test.shape[0]*100./N_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph of the network\n",
    "G = nx.from_scipy_sparse_matrix(adj)\n",
    "# pos = nx.spring_layout(G)\n",
    "plt.figure(figsize=(15,15))\n",
    "nx.draw(G, node_size=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation over  150 = 10 x 15 parameter settings\n",
      "Time taken:  32.426651001 0.540448315938\n",
      "Time taken:  35.4217522144 0.590363419056\n",
      "Time taken:  32.5707840919 0.542847216129\n",
      "Time taken:  28.3178520203 0.471967899799\n",
      "Time taken:  37.7108020782 0.628517035643\n",
      "Time taken:  33.8519601822 0.564199801286\n",
      "Time taken:  33.4277799129 0.557130066554\n",
      "Time taken:  35.2805221081 0.588013799985\n",
      "Time taken:  38.6531620026 0.644223499298\n",
      "Time taken:  38.2238948345 0.637065784136\n",
      "Time taken:  38.5173950195 0.641957199574\n",
      "Time taken:  37.4289009571 0.623815532525\n",
      "Time taken:  41.4279708862 0.690468132496\n",
      "Time taken:  39.0019080639 0.650032369296\n",
      "Time taken:  39.8033900261 0.663390767574\n",
      "Time taken:  38.6758730412 0.644598599275\n",
      "Time taken:  39.6296820641 0.66049536864\n",
      "Time taken:  34.161411047 0.569359819094\n",
      "Time taken:  38.2330408096 0.637217748165\n",
      "Time taken:  41.4957931042 0.691598848502\n",
      "Time taken:  38.0572099686 0.634287333488\n",
      "Time taken:  41.3895580769 0.689828316371\n",
      "Time taken:  44.6486189365 0.744144248962\n",
      "Time taken:  41.8822028637 0.698039913177\n",
      "Time taken:  44.0731720924 0.734555184841\n",
      "Time taken:  48.6442658901 0.810738333066\n",
      "Time taken:  45.2095251083 0.753492685159\n",
      "Time taken:  52.3589618206 0.872651847204\n",
      "Time taken:  49.648111105 0.827468951543\n",
      "Time taken:  49.9082949162 0.831805547078\n",
      "Time taken:  42.137141943 0.702286235491\n",
      "Time taken:  51.7097189426 0.861829050382\n",
      "Time taken:  36.8946399689 0.614913101991\n",
      "Time taken:  40.2086820602 0.670145467917\n",
      "Time taken:  42.3466489315 0.705781300863\n",
      "Time taken:  45.9085748196 0.765147916476\n",
      "Time taken:  43.3629918098 0.72271866401\n",
      "Time taken:  47.0044438839 0.783411081632\n",
      "Time taken:  50.151777029 0.835863900185\n",
      "Time taken:  49.2239308357 0.820399399598\n",
      "Time taken:  46.6413021088 0.777355500062\n",
      "Time taken:  50.4386341572 0.840644435088\n",
      "Time taken:  53.7910010815 0.896517248948\n",
      "Time taken:  65.6722211838 1.0945375363\n",
      "Time taken:  58.4248359203 0.973747781912\n",
      "Time taken:  46.5283808708 0.775477965673\n",
      "Time taken:  48.4749941826 0.807920002937\n",
      "Time taken:  41.3044171333 0.688408986727\n",
      "Time taken:  48.7673490047 0.81279433171\n",
      "Time taken:  51.0891339779 0.851486031214\n",
      "Time taken:  48.2551271915 0.80425260067\n",
      "Time taken:  48.007630825 0.80012759765\n",
      "Time taken:  56.7508580685 0.945848182837\n",
      "Time taken:  54.6303498745 0.910506498814\n",
      "Time taken:  58.3975300789 0.973294297854\n",
      "Time taken:  53.8095550537 0.896826549371\n",
      "Time taken:  59.6864290237 0.994777182738\n",
      "Time taken:  59.8765280247 0.997942713896\n",
      "Time taken:  71.6837480068 1.19472970168\n",
      "Time taken:  66.4471731186 1.10745555162\n",
      "Time taken:  45.732765913 0.762213349342\n",
      "Time taken:  53.140073061 0.885670200984\n",
      "Time taken:  47.2133800983 0.786892449856\n",
      "Time taken:  52.4282138348 0.873805050055\n",
      "Time taken:  53.881152153 0.898022019863\n",
      "Time taken:  57.9418878555 0.9657005469\n",
      "Time taken:  56.3100290298 0.938502935568\n",
      "Time taken:  59.1887221336 0.986480236053\n",
      "Time taken:  57.7418880463 0.962365651131\n",
      "Time taken:  61.0230929852 1.01705199877\n",
      "Time taken:  58.1046879292 0.968411898613\n",
      "Time taken:  67.7164878845 1.12860879898\n",
      "Time taken:  76.989757061 1.2831632177\n",
      "Time taken:  74.1925411224 1.23654446602\n",
      "Time taken:  74.0621240139 1.23436998526\n",
      "Time taken:  53.3176121712 0.888629702727\n",
      "Time taken:  57.7939019203 0.963232167562\n",
      "Time taken:  54.8619909286 0.914368951321\n",
      "Time taken:  58.416274786 0.973605982463\n",
      "Time taken:  60.1941819191 1.00323863427\n",
      "Time taken:  54.7270600796 0.912121065458\n",
      "Time taken:  59.1680779457 0.986135351658\n",
      "Time taken:  60.1946299076 1.00324691534\n",
      "Time taken:  65.761051178 1.09601798455\n",
      "Time taken:  67.4855060577 1.1247603337\n",
      "Time taken:  71.5579960346 1.19263776541\n",
      "Time taken:  69.5614039898 1.15936096509\n",
      "Time taken:  74.4399158955 1.24066785177\n",
      "Time taken:  85.1090960503 1.41848786672\n",
      "Time taken:  78.0954179764 1.30159433285\n",
      "Time taken:  57.569642067 0.95949823459\n",
      "Time taken:  64.5999581814 1.07667008638\n",
      "Time taken:  53.7124638557 0.89520829916\n",
      "Time taken:  60.4235420227 1.00706361532\n",
      "Time taken:  61.8039097786 1.0300657471\n",
      "Time taken:  62.2742810249 1.03790735006\n",
      "Time taken:  64.0622489452 1.06770470142\n",
      "Time taken:  64.8107120991 1.08018072049\n",
      "Time taken:  69.9248850346 1.16541550159\n",
      "Time taken:  75.6869139671 1.26145124833\n",
      "Time taken:  65.1998438835 1.08666876554\n",
      "Time taken:  80.1404960155 1.33567561706\n",
      "Time taken:  85.3699889183 1.42283801635\n",
      "Time taken:  75.7450909615 1.26241880258\n",
      "Time taken:  81.6965639591 1.36161208153\n",
      "Time taken:  72.811262846 1.21352151632\n",
      "Time taken:  68.0602419376 1.13433971802\n",
      "Time taken:  70.0157020092 1.16693208218\n",
      "Time taken:  63.298074007 1.05497074922\n",
      "Time taken:  65.4928598404 1.09155023098\n",
      "Time taken:  73.8201670647 1.23034046888\n",
      "Time taken:  65.7567059994 1.09594573577\n",
      "Time taken:  73.5299179554 1.22549943527\n",
      "Time taken:  74.3733041286 1.23955571651\n",
      "Time taken:  79.0279250145 1.31713496447\n",
      "Time taken:  82.4292590618 1.37382153273\n",
      "Time taken:  82.2523868084 1.37087571621\n",
      "Time taken:  78.7485609055 1.31247940063\n",
      "Time taken:  86.8132510185 1.446888117\n",
      "Time taken:  92.149379015 1.53582783143\n",
      "Time taken:  68.3627851009 1.1393802166\n",
      "Time taken:  74.4892029762 1.24149078131\n",
      "Time taken:  70.7176239491 1.1786293149\n",
      "Time taken:  69.5110859871 1.15852088531\n",
      "Time taken:  76.9937460423 1.28323150078\n",
      "Time taken:  77.2351560593 1.2872552832\n",
      "Time taken:  75.2421870232 1.2540372014\n",
      "Time taken:  76.7291259766 1.27882361809\n",
      "Time taken:  77.5889539719 1.2931515495\n",
      "Time taken:  91.0127930641 1.51688043276\n",
      "Time taken:  86.7338380814 1.44556603432\n",
      "Time taken:  86.6588959694 1.4443154335\n",
      "Time taken:  94.9052472115 1.58175511758\n",
      "Time taken:  96.2899179459 1.60485980113\n",
      "Time taken:  91.8661301136 1.53110265334\n",
      "Time taken:  67.9357299805 1.13226413329\n",
      "Time taken:  86.4974689484 1.44162496726\n",
      "Time taken:  79.4402689934 1.32400656541\n",
      "Time taken:  72.3401038647 1.20566896598\n",
      "Time taken:  75.6597251892 1.26099866629\n",
      "Time taken:  78.4885928631 1.30814379851\n",
      "Time taken:  76.6343948841 1.27724324862\n",
      "Time taken:  88.253923893 1.47090023359\n",
      "Time taken:  79.5628700256 1.32605028152\n",
      "Time taken:  92.8485250473 1.54747876724\n",
      "Time taken:  86.3582479954 1.43930479685\n",
      "Time taken:  92.0309839249 1.53388531605\n",
      "Time taken:  113.329596996 1.88884203434\n",
      "Time taken:  107.266652822 1.78778132995\n",
      "Time taken:  98.1226718426 1.63537846406\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# Baseline MLP, validate, DONE\n",
    "#############################################################################################################\n",
    "# Tune #hidden layers and #hidden units (same for each layer)\n",
    "# Dropout fixed\n",
    "\n",
    "epochs = 10000\n",
    "train_batch_size = len(X_train)\n",
    "val_batch_size = len(X_val)\n",
    "test_batch_size = len(X_test)\n",
    "\n",
    "N_runs = 100\n",
    "dropout = 0.5\n",
    "\n",
    "N_hl_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # range of numbers of hidden layers\n",
    "N_hu_range = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80] # range of numbers of units per hidden layer\n",
    "print \"Validation over \", len(N_hl_range) * len(N_hu_range), \"=\", len(N_hl_range), \"x\", len(N_hu_range), \"parameter settings\"\n",
    "vals = np.zeros((len(N_hl_range), len(N_hu_range)))\n",
    "vals_std = np.zeros((len(N_hl_range), len(N_hu_range)))\n",
    "\n",
    "for a, N_hl in enumerate(N_hl_range):\n",
    "    for b, N_hu in enumerate(N_hu_range):\n",
    "        st = time.time()\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(N_hu, activation='relu', kernel_initializer='he_uniform', input_dim=N_features))\n",
    "        model.add(Dropout(dropout))\n",
    "        for i in range(1, N_hl):\n",
    "            model.add(Dense(N_hu, activation='relu', kernel_initializer='he_uniform'))\n",
    "            model.add(Dropout(dropout))\n",
    "        model.add(Dense(N_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "        #print model.summary()\n",
    "        early_stop = EarlyStopping(monitor='val_acc', patience=10, verbose=0) # stop after 10 epochs without improvement in val_acc\n",
    "\n",
    "        vals_actual = []\n",
    "        for i in range(N_runs):\n",
    "            model.fit(X_train, y_train, epochs=epochs, batch_size=train_batch_size, \n",
    "                       validation_data = (X_val, y_val), verbose=0, callbacks=[early_stop])\n",
    "\n",
    "            vals_actual.append( model.evaluate(X_val, y_val, batch_size=val_batch_size, verbose=0)[1] )\n",
    "\n",
    "        vals[a][b] = np.mean(vals_actual)\n",
    "        vals_std[a][b] = np.std(vals_actual)\n",
    "        print \"Time taken: \", time.time()-st, (time.time()-st)/60. \n",
    "# Total ~ 1.5 hod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# Save the results ! IDs 0,1, are used\n",
    "# TODO save as 1\n",
    "ID = 0 # zeroth trial, smaller # of hidden sizes\n",
    "ID = 1 # first ok trial , more # of hidden sizes\n",
    "\n",
    "if os.path.exists('./../../../Dataset/baseline_' + str(ID) + '_Citeseer.npz'):\n",
    "    raise NameError(\"Set saveID not in use!\")\n",
    "np.savez('./../../../Dataset/baseline_' + str(ID) + '_Citeseer.npz', vals=vals, vals_std=vals_std, \n",
    "         N_hl_range=N_hl_range, N_hu_range=N_hu_range, N_runs=N_runs, dropout=dropout, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hidden layers range [ 1  2  3  4  5  6  7  8  9 10]\n",
      "# hidden units range [10 15 20 25 30 35 40 45 50 55 60 65 70 75 80]\n",
      "Optimal number of hidden layers:  3\n",
      "Optimal number of hidden units:  50\n",
      "Best validation accuracy:  0.556840002537 0.556840002537 (10, 15) 38\n",
      "dropout 0.5 \tN_runs 100\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# Load tuned parameters - from Baseline MLP validation on CITESEER dataset\n",
    "#############################################################################################################\n",
    "\n",
    "ID = 0 # zeroth trial,  smaller # of hidden sizes\n",
    "ID = 1 # first ok trial, higher # of hidden sizes\n",
    "\n",
    "data = np.load('./../../../Dataset/baseline_' + str(ID) + '_Citeseer.npz')\n",
    "vals = data['vals']\n",
    "vals_std = data['vals_std']\n",
    "N_hl_range = data['N_hl_range']\n",
    "N_hu_range = data['N_hu_range']\n",
    "N_runs = data['N_runs']\n",
    "dropout = data['dropout']\n",
    "epochs = data['epochs']\n",
    "\n",
    "train_batch_size = len(X_train)\n",
    "val_batch_size = len(X_val)\n",
    "test_batch_size = len(X_test)\n",
    "\n",
    "best_N_hl = N_hl_range[np.argmax(vals) // len(N_hu_range)]\n",
    "best_N_hu = N_hu_range[np.argmax(vals) % len(N_hu_range)]\n",
    "\n",
    "print \"# hidden layers range\", N_hl_range\n",
    "print \"# hidden units range\", N_hu_range\n",
    "print \"Optimal number of hidden layers: \", best_N_hl\n",
    "print \"Optimal number of hidden units: \",  best_N_hu\n",
    "print \"Best validation accuracy: \", np.max(vals), vals[np.argmax(vals) // len(N_hu_range), np.argmax(vals) % len(N_hu_range)], vals.shape, np.argmax(vals)\n",
    "print \"dropout\", dropout, \"\\tN_runs\", N_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0914600000158\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALoAAAEYCAYAAADrreygAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f614e76b510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline method: validation accuracy in terms of hidden layer size (x) and number of hidden units per layer (y)\n"
     ]
    }
   ],
   "source": [
    "vmin= np.min(vals)\n",
    "vmax = np.max(vals)\n",
    "# mid_val = 0.15\n",
    "# cmap = 'hot_r' #'coolwarm'\n",
    "# vmax = 0.695440005958\n",
    "# vmin = 0.0842999996617\n",
    "print vmin\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(vals.T) # , vmax = 0.6 , cmap=cmap\n",
    "plt.xticks(range(len(N_hl_range)), N_hl_range)\n",
    "plt.yticks(range(len(N_hu_range)), N_hu_range)\n",
    "plt.xlabel('# hidden layers')\n",
    "plt.ylabel('# hidden units per leayer')\n",
    "plt.colorbar(orientation=\"horizontal\", fraction=0.027)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print \"Baseline method: validation accuracy in terms of hidden layer size (x) and number of hidden units per layer (y)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_980 (Dense)            (None, 50)                185200    \n",
      "_________________________________________________________________\n",
      "dropout_829 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_981 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_830 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_982 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_831 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_983 (Dense)            (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 190,606\n",
      "Trainable params: 190,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Test set accuracy:  0.507599999309 +/- 0.00710351817853\n",
      "Baseline accuracy (chance level):  0.166666666667\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# Train and Test, again averaged over 100 runs with rand init\n",
    "#############################################################################################################\n",
    "\n",
    "# best_N_hl   # from above\n",
    "# best_N_hu\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(best_N_hu, activation='relu', kernel_initializer='he_uniform', input_dim=N_features))\n",
    "model.add(Dropout(dropout))\n",
    "for i in range(1, best_N_hl):\n",
    "    model.add(Dense(best_N_hu, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(N_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "print model.summary()\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=10, verbose=0) # stop after 10 epochs without improvement in val_acc\n",
    "\n",
    "test_acc = []\n",
    "for i in range(N_runs):\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=train_batch_size, \n",
    "               validation_data = (X_val, y_val), verbose=0, callbacks=[early_stop])\n",
    "\n",
    "    test_acc.append( model.evaluate(X_test, y_test, batch_size=test_batch_size, verbose=0)[1] )\n",
    "\n",
    "print \"Test set accuracy: \", np.mean(test_acc), \"+/-\", np.std(test_acc)\n",
    "print \"Baseline accuracy (chance level): \", 1./N_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test set accuracy:  0.507599999309 +/- 0.00710351817853\n",
    "Baseline accuracy (chance level):  0.166666666667\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
